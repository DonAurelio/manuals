# Product Creation

The datacube needs to know about the properies of the scenes produced for every satellite we are interested in. As every satellite takes different **measuremets**, then every image will have different data. The datacube needs to care about organizing data of images from the same satellite together. For each satellite a **Product** must be created in the datacube. Then scenes from the same satellite corresponds with the same datacube Product. 

Start the container created in the Fist section or run the following preconfigured container. If you use the second option you have to create the directory *datacube_shared*. 

```sh 
mkdir datacube_shared
cd datacube_shared
```

Copy the file **LC080050572018122501T1-SC20190910190636.tar.gz** into the *datacube_shared* directory. And run the container.

```sh 
docker run \
	-it \
	--name mycube \
	-e DATACUBE_CONFIG_PATH=/opt/custom-config.conf \
	-e DB_DATABASE=datacube  \
	-e DB_HOSTNAME=localhost \
	-e DB_USERNAME=cube  \
	-e DB_PASSWORD=cube  \
	-e DB_PORT=5432 \
	-v ${PWD}:/datacube_shared \
	donaurelio/opendatacube:1.6.1-base \
	bash
```

Once you are within the container use `sudo su cube` to change to **cubo** user 

```sh 
sudo su cubo
cd 
```

Export the following env variable

```sh 
echo DATACUBE_CONFIG_PATH=/opt/custom-config.conf
```

Start the database service

```sh 
sudo service postgresql start
datacube system init
```

Check datacube database connection

```sh 
sudo service postgresql start
datacube system init
datacube system check

Version:       1.6.1+0.gee3c3d81.dirty
Config files:  /opt/custom-config.conf
Host:          localhost:5432
Database:      datacube
User:          cube
Environment:   None
Index Driver:  default

Valid connection:	YES
```

## Create Source Storage and Datacube Storage

We wil organize the data (scenes) we want to index in the datacube in the **/source_storage**. The data indexed and ingested by the datacube will be placed in **/dc_storage** autimatically by the datacube.

```sh 
sudo mkdir /dc_storage /source_storage
sudo chown cube:root /dc_storage /source_storage
```

## Create a the product LS8_OLI_LASRC for Landsat 8 Scenes

To define a new Product, a **Product Definition Document** is required. This file will describe the data that will be contained in the Product, more information about this in [1]. A definition files is available on this repository.

```sh 
git clone https://github.com/DonAurelio/manuals.git
```
Copy the Landsat 8 files into the */dc_storage*

```sh 
cp -r manuals/open-datacube/products/LS8_OLI_LASRC /dc_storage/
```

Create the landsat 8 product

```sh 
datacube product add /dc_storage/LS8_OLI_LASRC/description_file.yml

Added "ls8_collections_sr_scene"
```

List the Products created on the datacube

```sh 
datacube product list

creation_time: null
description: Landsat 8 USGS Collection 1 Higher Level SR scene proessed using LaSRC.
    30m UTM based projection.
format: GeoTiff
id: 1
instrument: OLI_TIRS
label: null
lat: null
lon: null
name: ls8_collections_sr_scene
platform: LANDSAT_8
product_type: LaSRC
time: null
```

## Data Indexing and Ingestion

Scenes are indexed on the datacube database for ease acces such data through the datacube Python API. An optional step performed by the datacube is the ingestion. This step divide the scene in pieces called tiles. This pieces are stored in dc_stoarge and improve the performance on queries made to the datacube, more details in [2]. 

Create a folder for the landsat 8 scenes we are intended to index (and ingest) in */source_storage*

```sh 
mkdir /source_storage/LS8_OLI_LASRC
```

Copy the .tar.gz file you copied in /datacube_shared in /source_storage

```sh 
cp /datacube_shared/LC080050572018122501T1-SC20190910190636.tar.gz /source_storage/LS8_OLI_LASRC
```

Index an ingest the scene using the script provided in this repository.

```sh 
cp manuals/open-datacube/scripts/IngestBatch.sh ~
./IngestBatch.sh /source_storage/LS8_OLI_LASRC/ /dc_storage/LS8_OLI_LASRC/ingest_file.yml /dc_storage/LS8_OLI_LASRC/mgen_script.py
```

# References

1. [Product](https://datacube-core.readthedocs.io/en/latest/architecture/data_model.html#product)
2. [Indexing Data](https://datacube-core.readthedocs.io/en/latest/ops/indexing.html#indexing-data)

Install sudo apt install libgdal-dev